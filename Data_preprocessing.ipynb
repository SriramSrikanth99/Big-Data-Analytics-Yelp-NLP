{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "oqfUsOWV00df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext, SQLContext\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "E23ZL90Hinbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "conf = SparkConf()\n",
        "sc = SparkContext(conf=conf)\n",
        "sqlContext = SQLContext(sc)"
      ],
      "metadata": {
        "id": "KuVjeUPTilMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_rdd = sc.textFile(\"path to train.csv\")\n",
        "test_rdd = sc.textFile(\"path to test.csv\")\n",
        "\n",
        "# Convert to rdd\n",
        "def split(line):\n",
        "  label = int(line[1])\n",
        "  sentence = line[5:-1]\n",
        "  return label, sentence\n",
        "\n",
        "train_rdd = train_rdd.map(split)\n",
        "test_rdd = test_rdd.map(split)\n",
        "\n",
        "# Get the first 10 rows\n",
        "for row in train_rdd.take(10):\n",
        "  print(row)"
      ],
      "metadata": {
        "id": "Ig8B9SWhieJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Remove special characters\n",
        "pattern = r'[^a-zA-Z0-9\\s]'     # Define a pattern that only includes whitespaces and alphanumeric characters\n",
        "def remove_special_characters(text):\n",
        "    text = re.sub(pattern, ' ', text)     # Replace characters not belonging to the pattern with whitespace\n",
        "    return text.replace('\\n', ' ')      # Replace '\\n' with whitespace\n",
        "\n",
        "# Remove indepedent numbers and stopwords\n",
        "def is_number(s):\n",
        "    if s.isdigit():\n",
        "        return True\n",
        "    try:\n",
        "        float(s)\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.add('')\n",
        "def remove_numbers_and_stopwords(text):\n",
        "    return \" \".join([x for x in text.split(' ') if not is_number(x) and x not in stop_words])\n",
        "\n",
        "# Remove abundant spaces\n",
        "def remove_extra_spaces(text):\n",
        "    return \" \".join(text.split(' '))\n",
        "\n",
        "# Lemmatization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "def lemmatize(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split(\" \")])\n",
        "\n",
        "def preprocess(text):\n",
        "    text = remove_special_characters(text)\n",
        "    text = text.lower()\n",
        "    text = remove_numbers_and_stopwords(text)\n",
        "    text = remove_extra_spaces(text)\n",
        "    text = lemmatize(text)\n",
        "    return text\n",
        "\n",
        "# Remove rows with empty word lists\n",
        "def filter_empty_and_none(row):\n",
        "    return row[1] is not None and len(row[1]) > 0\n",
        "\n",
        "train_preprocessed_rdd = (\n",
        "    train_rdd\n",
        "    .map(lambda x : (x[0], preprocess(x[1])))\n",
        "    .filter(filter_empty_and_none)\n",
        ")\n",
        "test_preprocessed_rdd = (\n",
        "    test_rdd\n",
        "    .map(lambda x : (x[0], preprocess(x[1])))\n",
        "    .filter(filter_empty_and_none)\n",
        ")\n",
        "\n",
        "for row in test_preprocessed_rdd.take(10):\n",
        "  print(row)"
      ],
      "metadata": {
        "id": "AH86BL_71j4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()"
      ],
      "metadata": {
        "id": "7YldZGjw1dMS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}